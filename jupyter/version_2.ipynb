{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFinder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************\n",
    "*******************\n",
    "## Studying fast way to turn to strong typing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../src/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimethod import multidispatch, multimethod\n",
    "\n",
    "\n",
    "@multidispatch\n",
    "def disp(events, detectors, duration):\n",
    "    raise NotImplementedError(\"The type of input parameters is not supported\")\n",
    "\n",
    "\n",
    "@disp.register\n",
    "def _(events: str | list, detectors: str | list, duration: int): ...\n",
    "\n",
    "\n",
    "@multimethod\n",
    "def meth(events, detectors, duration):\n",
    "    raise NotImplementedError(\"The type of input parameters is not supported\")\n",
    "\n",
    "\n",
    "@meth.register\n",
    "def _(events: str | list, detectors: str | list, duration: int): ...\n",
    "\n",
    "from functools import singledispatch\n",
    "\n",
    "\n",
    "@singledispatch\n",
    "def single(events, detectors, duration):\n",
    "    raise NotImplementedError(\"The type of input parameters is not supported\")\n",
    "\n",
    "\n",
    "@single.register\n",
    "def _(events: str | list, detectors: str | list, duration: int): ...\n",
    "\n",
    "\n",
    "def manual(events, detectors, duration):\n",
    "    assert type(events) == str\n",
    "    assert type(detectors) == str\n",
    "    assert type(duration) == int\n",
    "\n",
    "\n",
    "def nothing(events: str | list, detectors: str | list, duration: int): ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit disp(\"hi\", \"\", 3)\n",
    "%timeit meth(\"\", \"\", 1)\n",
    "%timeit single(\"\", \"\", 1)\n",
    "%timeit manual(\"\", \"\", 1)\n",
    "%timeit nothing(\"\", \"\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "### The best option so far is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache, wraps\n",
    "from typing import (\n",
    "    TypeVar,\n",
    "    get_type_hints,\n",
    "    Callable,\n",
    "    GenericAlias,\n",
    "    Union,\n",
    "    List,\n",
    "    get_origin,\n",
    "    get_args,\n",
    ")\n",
    "import typing\n",
    "from types import UnionType\n",
    "from numpy import float32\n",
    "import numpy\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "import types\n",
    "\n",
    "\n",
    "def _check_arg(arg, arg_type_hints):\n",
    "    \"\"\"\n",
    "    Checks if an argument is an instance of one or more specified types.\n",
    "\n",
    "    Args:\n",
    "        arg: The argument to check.\n",
    "        arg_type_hints: Type hints specifying the type(s) the argument should have.\n",
    "            It can be a simple type or a composed type, such as a Union of types.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If the argument is not an instance of the specified type.\n",
    "        NotImplementedError: If type checking for a certain type is not implemented.\n",
    "\n",
    "    Examples:\n",
    "        # Check if 'value' is an integer\n",
    "        _check_arg(value, int)\n",
    "\n",
    "        # Check if 'data' is a list of integers\n",
    "        _check_arg(data, list[int])\n",
    "\n",
    "        # Check if 'value' is either a string or an integer\n",
    "        _check_arg(value, typing.Union[str, int])\n",
    "\n",
    "        # Check if 'matrix' is a list of lists of integers\n",
    "        _check_arg(matrix, list[list[int]])\n",
    "    \"\"\"\n",
    "    type_error_msg = f\"{arg} is not an instance of {arg_type_hints}\"\n",
    "    arg_type_origin = typing.get_origin(arg_type_hints)\n",
    "\n",
    "    # Check for simple types\n",
    "    if arg_type_origin is None:\n",
    "        error_formats = [dict, list]\n",
    "        if arg_type_hints in error_formats:\n",
    "            raise NotImplementedError(\n",
    "                f\"Type checking for {arg_type_hints} not implemented\"\n",
    "            )\n",
    "        if not isinstance(arg, arg_type_hints):\n",
    "            raise TypeError(type_error_msg)\n",
    "\n",
    "    # Check for composed types\n",
    "    else:\n",
    "        arg_types = typing.get_args(arg_type_hints)\n",
    "        if arg_type_origin not in (typing.Union, types.UnionType):\n",
    "            non_monadic = (\n",
    "                any(typing.get_origin(_) for _ in arg_types) or len(arg_types) > 1\n",
    "            )\n",
    "            if non_monadic:\n",
    "                raise NotImplementedError(\n",
    "                    f\"Type checking for {arg_type_hints} not implemented: Non monadic!\"\n",
    "                )\n",
    "            # Checking if parent object is right\n",
    "            if not isinstance(arg, arg_type_origin):\n",
    "                raise TypeError(type_error_msg)\n",
    "            if hasattr(arg, \"dtype\"):\n",
    "                if arg.dtype not in arg_types:\n",
    "                    raise TypeError(type_error_msg)\n",
    "            elif isinstance(arg, list):\n",
    "                if not all(isinstance(elem, arg_types) for elem in arg):\n",
    "                    raise TypeError(type_error_msg)\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    f\"Type checking for {arg_types} not implemented\"\n",
    "                )\n",
    "        else:\n",
    "            for arg_type in arg_types:\n",
    "                try:\n",
    "                    _check_arg(arg, arg_type)\n",
    "                    break\n",
    "                except TypeError:\n",
    "                    continue\n",
    "                except NotImplementedError as err:\n",
    "                    raise err\n",
    "            else:\n",
    "                raise TypeError(type_error_msg)\n",
    "\n",
    "\n",
    "def type_check(func: Callable[..., T]) -> Callable[..., T]:\n",
    "    var_name_and_type = get_type_hints(func)\n",
    "    var_names = list(var_name_and_type.keys())\n",
    "\n",
    "    @wraps(func)\n",
    "    def type_checker(*args, **kwargs):\n",
    "        for i, arg in enumerate(args):\n",
    "            # positional arguments are by default in the right order\n",
    "            arg_type_hints = var_name_and_type[var_names[i]]\n",
    "            _check_arg(arg, arg_type_hints)\n",
    "\n",
    "        for kwarg_name, kwarg in kwargs.items():\n",
    "            kwarg_type = var_name_and_type[kwarg_name]\n",
    "            _check_arg(kwarg, kwarg_type)\n",
    "\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return type_checker\n",
    "\n",
    "\n",
    "@type_check\n",
    "def manual_dec(\n",
    "    events: str | list[int],\n",
    "    detectors: str | int,\n",
    "    duration: int,\n",
    "    dt: numpy.ndarray[numpy.int32],\n",
    "    alpha: float = 0.1,\n",
    "): ...\n",
    "\n",
    "\n",
    "# Esempi di utilizzo\n",
    "manual_dec([2], 3, 3, numpy.array(5), alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyburst import type_check\n",
    "\n",
    "\n",
    "@type_check\n",
    "def manual_dec(\n",
    "    events: list[str],\n",
    "    detectors: str,\n",
    "    duration: int,\n",
    "    dt: float,\n",
    "    alpha: float,\n",
    "): ...\n",
    "\n",
    "\n",
    "manual_dec([\"hi\"], \"\", duration=3, dt=0.5, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "-------------\n",
    "## Timeseries data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../src/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un primo approccio potrebbe essere il seguente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from warnings import warn\n",
    "\n",
    "import numpy\n",
    "import cupy\n",
    "import pandas\n",
    "\n",
    "\n",
    "def _obj_size(obj):\n",
    "    \"\"\"\n",
    "    Calculate the total size in bytes of a nested object, considering numpy.ndarray, cupy.ndarray, and pandas.Series objects as well.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : dict or list or tuple or numpy.ndarray or cupy.ndarray or pandas.Series\n",
    "        The nested object to calculate the size of.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The total size of the nested object in bytes.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> nested_dict = {\n",
    "    ...     'a': 1,\n",
    "    ...     'b': {'c': 2, 'd': 3},\n",
    "    ...     'e': {'f': {'g': 4, 'h': 5}},\n",
    "    ...     'numpy_array': numpy.zeros((10, 10)),\n",
    "    ...     'cupy_array': cupy.zeros((10, 10)),\n",
    "    ...     'pandas_series': pandas.Series(range(10))\n",
    "    ... }\n",
    "    >>> size_in_bytes = _obj_size(nested_dict)\n",
    "    >>> print(\"Total size of the nested object:\", size_in_bytes, \"bytes\")\n",
    "    Total size of the nested object: XXX bytes\n",
    "    \"\"\"\n",
    "    total_size = 0\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            if isinstance(value, dict):\n",
    "                total_size += _obj_size(value)\n",
    "            else:\n",
    "                total_size += sys.getsizeof(value)\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        for item in obj:\n",
    "            total_size += _obj_size(item)\n",
    "    elif isinstance(obj, (numpy.ndarray, cupy.ndarray, pandas.Series)):\n",
    "        total_size += obj.nbytes\n",
    "    else:\n",
    "        total_size += sys.getsizeof(obj)\n",
    "\n",
    "    return total_size\n",
    "\n",
    "\n",
    "class LRUCache(OrderedDict):\n",
    "    \"\"\"\n",
    "    A Least Recently Used (LRU) cache implemented using an OrderedDict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_size_mb : float, optional\n",
    "        The maximum size of the cache in megabytes (default is 0.0001 MB).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    nbytes : int\n",
    "        The current size of the cache in bytes.\n",
    "    cache_size_mb : float\n",
    "        The maximum size of the cache in megabytes.\n",
    "    cache_contents : list\n",
    "        The keys present in the cache.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The cache automatically evicts the least recently used items when the size limit is reached.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> cache = LRUCache(max_size_mb=1)\n",
    "    >>> cache['a'] = 1\n",
    "    >>> cache['b'] = 2\n",
    "    >>> cache['c'] = 3\n",
    "    >>> print(cache.cache_contents)\n",
    "    ['a', 'b', 'c']\n",
    "    >>> print(cache.nbytes)\n",
    "    XXX bytes\n",
    "    >>> print(cache.cache_size_mb)\n",
    "    1.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_size_mb: float = 100):\n",
    "        super().__init__()\n",
    "        self.max_size_bytes = max_size_mb * (1024 * 1024)  # Convert MB to bytes\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if key in self:\n",
    "            value = super().__getitem__(key)\n",
    "            self.move_to_end(key)  # Update the item as the most recent\n",
    "            return value\n",
    "        else:\n",
    "            raise KeyError(key)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key not in self:\n",
    "            item_size_bytes = _obj_size(value)\n",
    "            while self._get_size() + item_size_bytes > self.max_size_bytes:\n",
    "                self._evict_least_recently_used()\n",
    "        super().__setitem__(key, value)\n",
    "\n",
    "    def _get_size(self):\n",
    "        return _obj_size(self)\n",
    "\n",
    "    def _evict_least_recently_used(self):\n",
    "        key, _ = self.popitem(last=False)  # Remove the least recent item\n",
    "        warn(f\"Removed {key} to free cache space.\")\n",
    "\n",
    "    @property\n",
    "    def nbytes(self):\n",
    "        \"\"\"\n",
    "        int: The current size of the cache in bytes.\n",
    "        \"\"\"\n",
    "        return self._get_size()\n",
    "\n",
    "    @property\n",
    "    def cache_size_mb(self):\n",
    "        \"\"\"\n",
    "        float: The maximum size of the cache in megabytes.\n",
    "        \"\"\"\n",
    "        return self.max_size_bytes / (1024 * 1024)\n",
    "\n",
    "    @property\n",
    "    def cache_contents(self):\n",
    "        \"\"\"\n",
    "        list: The keys present in the cache.\n",
    "        \"\"\"\n",
    "        return list(self.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m             rows\u001b[38;5;241m.\u001b[39mappend(row)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tabulate(rows, headers\u001b[38;5;241m=\u001b[39mheaders, tablefmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfancy_grid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(nested_dict_to_table(\u001b[43md\u001b[49m, headers))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "headers = [\"cache_key_1\", \"cache_key_2\",\"event_name\", \"detector_name\", \"timeseries\", \"gps_time\", \"duration\"]\n",
    "def nested_dict_to_table(nested_dict, headers):\n",
    "    rows = []\n",
    "    for event_key, value in nested_dict.items():\n",
    "        for detector_key, data in value.items():\n",
    "            row = [event_key, detector_key]\n",
    "            for k, v in data.items():\n",
    "                if isinstance(v, list):\n",
    "                    v = f\"<TimeSeries(...)>\"\n",
    "                row.append(v)\n",
    "            rows.append(row)\n",
    "    return tabulate(rows, headers=headers, tablefmt=\"fancy_grid\")\n",
    "\n",
    "\n",
    "print(nested_dict_to_table(d, headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voglio provare a creare una classe che gestisca in maniera specifica il tipo di dati di cui ho bisogno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyburst._typing import type_check\n",
    "from pyburst._data_loader import LRUCache\n",
    "from gwpy.timeseries import TimeSeries\n",
    "import gwosc.datasets\n",
    "from warnings import warn\n",
    "\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "class EventDataLoader:\n",
    "    _AVAILABLE_SOURCES = {\n",
    "        \"remote_open\": \"_fetch_remote\",\n",
    "        \"local\": \"_fetch_local\",\n",
    "    }\n",
    "\n",
    "    _CACHED_DATA = LRUCache()\n",
    "\n",
    "    @classmethod\n",
    "    @type_check(classmethod=True)\n",
    "    def _validate_source(cls, source: str):\n",
    "        if source not in cls._AVAILABLE_SOURCES.keys():\n",
    "            raise NotImplementedError(f\"{source} is not a valid source.\")\n",
    "\n",
    "    @classmethod\n",
    "    @type_check(classmethod=True)\n",
    "    def _fetch_remote(\n",
    "        cls,\n",
    "        event_name: str,\n",
    "        detector_name: str,\n",
    "        duration: float,\n",
    "        sample_rate: int,\n",
    "        url: str,\n",
    "        format: str,\n",
    "        max_attempts: int,\n",
    "        this_attempt: int = 1,\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        try:\n",
    "            event_gps_time = gwosc.datasets.event_gps(event_name)\n",
    "            start_time = event_gps_time - duration / 2\n",
    "            end_time = event_gps_time + duration / 2\n",
    "            signal = TimeSeries.fetch_open_data(\n",
    "                detector_name,\n",
    "                start_time,\n",
    "                end_time,\n",
    "                sample_rate,\n",
    "                format=format,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "            result = {\n",
    "                \"event_name\": event_name,\n",
    "                \"detector_name\": detector_name,\n",
    "                \"time_series\": signal,\n",
    "                \"gps_time\": event_gps_time,\n",
    "                \"duration\": duration,\n",
    "            }\n",
    "            return result\n",
    "        except:\n",
    "            if this_attempt < max_attempts:\n",
    "                warn(\n",
    "                    f\"Failed downloading {this_attempt}/{max_attempts} times, retrying...\",\n",
    "                    ResourceWarning,\n",
    "                )\n",
    "                cls._fetch_remote(\n",
    "                    event_name,\n",
    "                    detector_name,\n",
    "                    duration,\n",
    "                    sample_rate,\n",
    "                    format,\n",
    "                    max_attempts,\n",
    "                    this_attempt + 1,\n",
    "                )\n",
    "            else:\n",
    "                raise ConnectionError(\n",
    "                    f\"Failed downloading too many times ({this_attempt})\"\n",
    "                )\n",
    "\n",
    "    @classmethod\n",
    "    @type_check(classmethod=True)\n",
    "    def _fetch_local(\n",
    "        cls,\n",
    "        event_name: str,\n",
    "        detector_name: str,\n",
    "        duration: float,\n",
    "        sample_rate: int,\n",
    "        url: str,\n",
    "        format: str,\n",
    "        max_attempts: int,\n",
    "        this_attempt: int = 1,\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        try:\n",
    "            event_gps_time = gwosc.datasets.event_gps(event_name)\n",
    "            start_time = event_gps_time - duration / 2\n",
    "            end_time = event_gps_time + duration / 2\n",
    "            signal = TimeSeries.fetch_open_data(\n",
    "                detector_name,\n",
    "                start_time,\n",
    "                end_time,\n",
    "                sample_rate,\n",
    "                format=format,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "            result = {\n",
    "                \"event_name\": event_name,\n",
    "                \"detector_name\": detector_name,\n",
    "                \"time_series\": signal,\n",
    "                \"gps_time\": event_gps_time,\n",
    "                \"duration\": duration,\n",
    "            }\n",
    "            return result\n",
    "        except:\n",
    "            if this_attempt < max_attempts:\n",
    "                warn(\n",
    "                    f\"Failed downloading {this_attempt}/{max_attempts} times, retrying...\",\n",
    "                    ResourceWarning,\n",
    "                )\n",
    "                cls._fetch_remote(\n",
    "                    event_name,\n",
    "                    detector_name,\n",
    "                    duration,\n",
    "                    sample_rate,\n",
    "                    format,\n",
    "                    max_attempts,\n",
    "                    this_attempt + 1,\n",
    "                )\n",
    "            else:\n",
    "                raise ConnectionError(\n",
    "                    f\"Failed downloading too many times ({this_attempt})\"\n",
    "                )\n",
    "\n",
    "    @classmethod\n",
    "    @type_check(classmethod=True)\n",
    "    def _save_event(cls, event_data: dict, save_path: str, fmt: str):\n",
    "        event_name = event_data[\"event_name\"]\n",
    "        detector_name = event_data[\"detector_name\"]\n",
    "        gps_time = event_data[\"gps_time\"]\n",
    "        timeseries = event_data[\"time_series\"]\n",
    "        file_path = os.path.join(save_path, event_name, detector_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            os.makedirs(\n",
    "                file_path,\n",
    "            )\n",
    "        timeseries.write(\n",
    "            file_path + f\"/{event_name}_{detector_name}_{gps_time}_.{fmt}\",\n",
    "            format=fmt,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    @type_check(classmethod=True)\n",
    "    def save_event_data(cls, data_dict: dict, save_path: str, fmt: str = \"hdf5\"):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.submit(\n",
    "                    cls._save_event,\n",
    "                    data_dict[event_name][detector_name],\n",
    "                    save_path,\n",
    "                    fmt,\n",
    "                )\n",
    "                for event_name in data_dict.keys()\n",
    "                for detector_name in data_dict[event_name]\n",
    "            ]\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "\n",
    "    @classmethod\n",
    "    @type_check(classmethod=True)\n",
    "    def get_event_data(\n",
    "        cls,\n",
    "        event_names: list[str],\n",
    "        detector_names: list[str],\n",
    "        duration: float = 50.0,\n",
    "        source: str = \"remote_open\",\n",
    "        url: str = \"\",\n",
    "        sample_rate: int = 4096,\n",
    "        format: str = \"hdf5\",\n",
    "        max_attempts: int = 100,\n",
    "        cache_results: bool = True,\n",
    "        force_cache_overwrite: bool = False,\n",
    "        parallel: bool = True,\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        # checking if source is supported\n",
    "        cls._validate_source(source)\n",
    "\n",
    "        # getting the correct fetch function depending on input\n",
    "        _fetch_function = getattr(cls, cls._AVAILABLE_SOURCES[source])\n",
    "\n",
    "        if parallel:\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                futures = [\n",
    "                    executor.submit(\n",
    "                        _fetch_function,\n",
    "                        event_name,\n",
    "                        detector_name,\n",
    "                        duration,\n",
    "                        sample_rate,\n",
    "                        url,\n",
    "                        format,\n",
    "                        max_attempts,\n",
    "                        1,\n",
    "                        False,\n",
    "                    )\n",
    "                    for event_name in event_names\n",
    "                    for detector_name in detector_names\n",
    "                    if event_name not in cls._CACHED_DATA\n",
    "                    or detector_name not in cls._CACHED_DATA[event_name]\n",
    "                    or force_cache_overwrite\n",
    "                    or cls._CACHED_DATA[event_name][detector_name][\"duration\"]\n",
    "                    != duration\n",
    "                ]\n",
    "\n",
    "                for future in futures:\n",
    "                    result = future.result()\n",
    "                    event_name, detector_name = (\n",
    "                        result[\"event_name\"],\n",
    "                        result[\"detector_name\"],\n",
    "                    )\n",
    "                    cls._CACHED_DATA.setdefault(event_name, {})[detector_name] = result\n",
    "\n",
    "        else:\n",
    "            for event_name in event_names:\n",
    "                for detector_name in detector_names:\n",
    "                    if (\n",
    "                        event_name not in cls._CACHED_DATA\n",
    "                        or detector_name not in cls._CACHED_DATA[event_name]\n",
    "                        or force_cache_overwrite\n",
    "                        or cls._CACHED_DATA[event_name][detector_name][\"duration\"]\n",
    "                        != duration\n",
    "                    ):\n",
    "                        cls._CACHED_DATA.setdefault(event_name, {})[detector_name] = (\n",
    "                            _fetch_function(\n",
    "                                event_name,\n",
    "                                detector_name,\n",
    "                                duration,\n",
    "                                sample_rate,\n",
    "                                format,\n",
    "                                max_attempts,\n",
    "                                1,\n",
    "                                verbose,\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "        out_var = dict(cls._CACHED_DATA)\n",
    "        if not cache_results:\n",
    "            cls._CACHED_DATA.clear()\n",
    "\n",
    "        return out_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = EventDataLoader.get_event_data(\n",
    "    [\"GW150914-v3\"],\n",
    "    [\"L1\", \"H1\"],\n",
    "    10.,\n",
    "    \"remote_open\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File exists: .\\GW150914-v3\\L1/GW150914-v3_L1_1126259462.4_.hdf5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mEventDataLoader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_event_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\coding\\python\\qptransform\\qp-transform\\jupyter\\../src\\pyburst\\_typing.py:124\u001b[0m, in \u001b[0;36mtype_check.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     kwarg_type \u001b[38;5;241m=\u001b[39m var_name_and_type[kwarg_name]\n\u001b[0;32m    122\u001b[0m     _check_types(kwarg, kwarg_type)\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 162\u001b[0m, in \u001b[0;36mEventDataLoader.save_event_data\u001b[1;34m(cls, data_dict, save_path, fmt)\u001b[0m\n\u001b[0;32m    151\u001b[0m futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    152\u001b[0m     executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_save_event,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m detector_name \u001b[38;5;129;01min\u001b[39;00m data_dict[event_name]\n\u001b[0;32m    160\u001b[0m ]\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[1;32m--> 162\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.752.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\coding\\python\\qptransform\\qp-transform\\jupyter\\../src\\pyburst\\_typing.py:124\u001b[0m, in \u001b[0;36mtype_check.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     kwarg_type \u001b[38;5;241m=\u001b[39m var_name_and_type[kwarg_name]\n\u001b[0;32m    122\u001b[0m     _check_types(kwarg, kwarg_type)\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 143\u001b[0m, in \u001b[0;36mEventDataLoader._save_event\u001b[1;34m(cls, event_data, save_path, fmt)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file_path):\n\u001b[0;32m    142\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(file_path, )\n\u001b[1;32m--> 143\u001b[0m \u001b[43mtimeseries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mevent_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdetector_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgps_time\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfmt\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmt\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gwpy\\timeseries\\core.py:327\u001b[0m, in \u001b[0;36mTimeSeriesBase.write\u001b[1;34m(self, target, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, target, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    313\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Write this `TimeSeries` to a file\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m    Notes\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m    -----\"\"\"\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\astropy\\io\\registry\\compat.py:52\u001b[0m, in \u001b[0;36m_make_io_func.<locals>.wrapper\u001b[1;34m(registry, *args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m     registry \u001b[38;5;241m=\u001b[39m default_registry\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# get and call bound method from registry instance\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\astropy\\io\\registry\\core.py:386\u001b[0m, in \u001b[0;36mUnifiedOutputRegistry.write\u001b[1;34m(self, data, format, *args, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_valid_format(\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, path, fileobj, args, kwargs\n\u001b[0;32m    383\u001b[0m     )\n\u001b[0;32m    385\u001b[0m writer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_writer(\u001b[38;5;28mformat\u001b[39m, data\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m--> 386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gwpy\\types\\io\\hdf5.py:234\u001b[0m, in \u001b[0;36mwrite_hdf5_series\u001b[1;34m(series, output, path, attrs, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attrs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     attrs \u001b[38;5;241m=\u001b[39m format_index_array_attrs(series)\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrite_hdf5_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gwpy\\io\\hdf5.py:124\u001b[0m, in \u001b[0;36mwith_write_hdf5.<locals>.decorated_func\u001b[1;34m(obj, fobj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m overwrite \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(fobj) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (overwrite \u001b[38;5;129;01mor\u001b[39;00m append):\n\u001b[1;32m--> 124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(fobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m append \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m h5f:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(obj, h5f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mOSError\u001b[0m: File exists: .\\GW150914-v3\\L1/GW150914-v3_L1_1126259462.4_.hdf5"
     ]
    }
   ],
   "source": [
    "EventDataLoader.save_event_data(data, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data locally"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
